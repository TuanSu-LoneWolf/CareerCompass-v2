# rag_engine.py
# RAG engine dÃ¹ng Gemini API (Google Generative AI)
# DÃ nh cho demo local â€” chá»‰ cáº§n .env cÃ³ GEMINI_API_KEY
# Náº¿u FAISS index Ä‘Ã£ build, sáº½ tá»± load Ä‘á»ƒ trÃ¡nh tá»‘n quota embedding

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# -----------------------
# Load environment
# -----------------------
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DOTENV_PATH = os.path.join(BASE_DIR, ".env")
load_dotenv(DOTENV_PATH)

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("âŒ Missing GEMINI_API_KEY in .env file")

os.environ["GEMINI_API_KEY"] = api_key  # LangChain váº«n dÃ¹ng biáº¿n nÃ y ná»™i bá»™

# -----------------------
# Paths
# -----------------------
PDF_FOLDER = os.path.join(BASE_DIR, "Data", "rag", "pdfs")
FAISS_INDEX_DIR = os.path.join(BASE_DIR, "Data", "rag", "faiss_index")

os.makedirs(PDF_FOLDER, exist_ok=True)
os.makedirs(FAISS_INDEX_DIR, exist_ok=True)

# -----------------------
# Step 1: Load & split PDFs
# -----------------------
def load_all_pdfs(folder_path):
    """Load táº¥t cáº£ PDF trong thÆ° má»¥c."""
    documents = []
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            path = os.path.join(folder_path, filename)
            loader = PyPDFLoader(path)
            documents.extend(loader.load())
    return documents

def prepare_docs():
    print("ðŸ“„ Äang load tÃ i liá»‡u PDF...")
    raw_docs = load_all_pdfs(PDF_FOLDER)
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs_local = splitter.split_documents(raw_docs)
    print(f"âœ… Tá»•ng sá»‘ Ä‘oáº¡n: {len(docs_local)}")
    return docs_local

# Táº¡m Ä‘á»ƒ docs = None, chá»‰ load náº¿u cáº§n build
docs = None

# -----------------------
# Step 2: FAISS + Embeddings (with auto-detect + rebuild on dim mismatch)
# -----------------------
import shutil
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings

API_KEY = os.getenv("GEMINI_API_KEY")

embedding_model = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=API_KEY
)

def build_faiss_from_docs():
    docs_local = prepare_docs()
    print("ðŸš€ Báº¯t Ä‘áº§u build FAISS index (sáº½ tá»‘n request embedding)...")
    vs = FAISS.from_documents(docs_local, embedding_model)

    print("ðŸš€ Báº¯t Ä‘áº§u build FAISS index (sáº½ tá»‘n request embedding)...")
    vs = FAISS.from_documents(docs, embedding_model)
    vs.save_local(FAISS_INDEX_DIR)
    print("ðŸ’¾ ÄÃ£ lÆ°u FAISS index.")
    return vs

vectorstore = None
index_path = os.path.join(FAISS_INDEX_DIR, "index.faiss")

if os.path.exists(index_path):
    print("ðŸ“¦ TÃ¬m tháº¥y FAISS index, Ä‘ang load...")
    try:
        vectorstore = FAISS.load_local(
            FAISS_INDEX_DIR,
            embedding_model,
            allow_dangerous_deserialization=True,
        )
        # Kiá»ƒm tra nhanh: thá»­ cháº¡y 1 truy váº¥n tÃ¬m kiáº¿m Ä‘á»ƒ phÃ¡t hiá»‡n mismatch dim
        try:
            # thá»­ tÃ¬m 1 káº¿t quáº£; náº¿u dimension mismatch sáº½ nÃ©m AssertionError tá»« faiss
            _ = vectorstore.similarity_search("kiá»ƒm tra dimension", k=1)
            print("âœ… FAISS index há»£p lá»‡ vá»›i embedding hiá»‡n táº¡i.")
        except AssertionError:
            print("âš ï¸ PhÃ¡t hiá»‡n mismatch dimension giá»¯a index vÃ  embedding. Rebuild FAISS...")
            shutil.rmtree(FAISS_INDEX_DIR)
            os.makedirs(FAISS_INDEX_DIR, exist_ok=True)
            vectorstore = build_faiss_from_docs()
    except Exception as e:
        print("âŒ Lá»—i khi load FAISS index:", e)
        print("âž¡ï¸ XÃ³a vÃ  rebuild láº¡i FAISS index Ä‘á»ƒ an toÃ n.")
        if os.path.exists(FAISS_INDEX_DIR):
            shutil.rmtree(FAISS_INDEX_DIR)
            os.makedirs(FAISS_INDEX_DIR, exist_ok=True)
        vectorstore = build_faiss_from_docs()
else:
    # chÆ°a cÃ³ index â†’ build má»›i
    vectorstore = build_faiss_from_docs()

# -----------------------
# Step 3: Model + Prompt
# -----------------------
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.1, google_api_key=os.getenv("GEMINI_API_KEY"))

prompt_template = PromptTemplate.from_template(
    """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn vá» tÆ° váº¥n hÆ°á»›ng nghiá»‡p.
DÆ°á»›i Ä‘Ã¢y lÃ  ngá»¯ cáº£nh tá»« tÃ i liá»‡u mÃ  báº¡n cÃ³ thá»ƒ tham kháº£o:

{context}

CÃ¢u há»i: {question}

HÃ£y tráº£ lá»i chi tiáº¿t, tá»± nhiÃªn vÃ  chÃ­nh xÃ¡c. Náº¿u khÃ´ng Ä‘á»§ dá»¯ kiá»‡n, hÃ£y pháº£n há»“i chung mang tÃ­nh Ä‘á»‹nh hÆ°á»›ng nghá» nghiá»‡p."""
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    chain_type_kwargs={"prompt": prompt_template},
)

# -----------------------
# Step 4: HÃ m truy váº¥n
# -----------------------
def answer_question(question: str) -> str:
<<<<<<< HEAD
    """
    Tráº£ vá» cÃ¢u tráº£ lá»i tá»« RAG.
    CÃ³ giá»›i háº¡n:
    - input tá»‘i Ä‘a 200 token
    - output tá»‘i Ä‘a 300 token
    """
    try:
        # cáº¯t input náº¿u quÃ¡ dÃ i (â‰ˆ 200 token ~ 800 kÃ½ tá»±)
        if len(question) > 800:
            question = question[:800] + "..."

        result = qa_chain.invoke({"question": question})  # âœ… Ä‘Ãºng key
        return result["result"]
    except Exception as e:
        print("âŒ Lá»—i khi gá»i RAG:", e)
        return "Xin lá»—i, cÃ³ lá»—i khi xá»­ lÃ½ yÃªu cáº§u cá»§a báº¡n."
=======
    """Tráº£ vá» cÃ¢u tráº£ lá»i tá»« há»‡ thá»‘ng RAG (cÃ³ debug chi tiáº¿t)."""
    print(f"ðŸ§  Nháº­n cÃ¢u há»i: {question}")
    try:
        result = qa_chain.invoke({"query": question})
        print(f"ðŸ§© Raw result type: {type(result)}")
        print(f"ðŸ§© Raw result content: {result}")

        if isinstance(result, dict):
            return result.get("result") or result.get("output_text") or str(result)
        elif hasattr(result, "output_text"):
            return result.output_text
        else:
            return str(result)

    except Exception as e:
        import traceback
        print("âŒ FULL ERROR TRACEBACK:")
        traceback.print_exc()
        print(f"âŒ Error message: {e}")
        return "Xin lá»—i, há»‡ thá»‘ng gáº·p lá»—i khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n."

>>>>>>> 4636e35 (fix counseling-chatbot)


