# rag_engine.py
# RAG engine d√πng Gemini API (Google Generative AI)
# D√†nh cho demo local ‚Äî ch·ªâ c·∫ßn .env c√≥ GEMINI_API_KEY
# N·∫øu FAISS index ƒë√£ build, s·∫Ω t·ª± load ƒë·ªÉ tr√°nh t·ªën quota embedding

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# -----------------------
# Load environment
# -----------------------
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DOTENV_PATH = os.path.join(BASE_DIR, ".env")
load_dotenv(DOTENV_PATH)

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("‚ùå Missing GEMINI_API_KEY in .env file")

os.environ["GEMINI_API_KEY"] = api_key  # LangChain v·∫´n d√πng bi·∫øn n√†y n·ªôi b·ªô

# -----------------------
# Paths
# -----------------------
PDF_FOLDER = os.path.join(BASE_DIR, "Data", "rag", "pdfs")
FAISS_INDEX_DIR = os.path.join(BASE_DIR, "Data", "rag", "faiss_index")

os.makedirs(PDF_FOLDER, exist_ok=True)
os.makedirs(FAISS_INDEX_DIR, exist_ok=True)

# -----------------------
# Step 1: Load & split PDFs
# -----------------------
def load_all_pdfs(folder_path):
    """Load t·∫•t c·∫£ PDF trong th∆∞ m·ª•c."""
    documents = []
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(".pdf"):
            path = os.path.join(folder_path, filename)
            loader = PyPDFLoader(path)
            documents.extend(loader.load())
    return documents

def prepare_docs():
    print("üìÑ ƒêang load t√†i li·ªáu PDF...")
    raw_docs = load_all_pdfs(PDF_FOLDER)
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs_local = splitter.split_documents(raw_docs)
    print(f"‚úÖ T·ªïng s·ªë ƒëo·∫°n: {len(docs_local)}")
    return docs_local

# T·∫°m ƒë·ªÉ docs = None, ch·ªâ load n·∫øu c·∫ßn build
docs = None

# -----------------------
# Step 2: FAISS + Embeddings (with auto-detect + rebuild on dim mismatch)
# -----------------------
import shutil
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings

API_KEY = os.getenv("GEMINI_API_KEY")

embedding_model = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=API_KEY
)

def build_faiss_from_docs():
    docs_local = prepare_docs()
    print("üöÄ B·∫Øt ƒë·∫ßu build FAISS index (s·∫Ω t·ªën request embedding)...")
    vs = FAISS.from_documents(docs_local, embedding_model)

    print("üöÄ B·∫Øt ƒë·∫ßu build FAISS index (s·∫Ω t·ªën request embedding)...")
    vs = FAISS.from_documents(docs, embedding_model)
    vs.save_local(FAISS_INDEX_DIR)
    print("üíæ ƒê√£ l∆∞u FAISS index.")
    return vs

vectorstore = None
index_path = os.path.join(FAISS_INDEX_DIR, "index.faiss")

if os.path.exists(index_path):
    print("üì¶ T√¨m th·∫•y FAISS index, ƒëang load...")
    try:
        vectorstore = FAISS.load_local(
            FAISS_INDEX_DIR,
            embedding_model,
            allow_dangerous_deserialization=True,
        )
        # Ki·ªÉm tra nhanh: th·ª≠ ch·∫°y 1 truy v·∫•n t√¨m ki·∫øm ƒë·ªÉ ph√°t hi·ªán mismatch dim
        try:
            # th·ª≠ t√¨m 1 k·∫øt qu·∫£; n·∫øu dimension mismatch s·∫Ω n√©m AssertionError t·ª´ faiss
            _ = vectorstore.similarity_search("ki·ªÉm tra dimension", k=1)
            print("‚úÖ FAISS index h·ª£p l·ªá v·ªõi embedding hi·ªán t·∫°i.")
        except AssertionError:
            print("‚ö†Ô∏è Ph√°t hi·ªán mismatch dimension gi·ªØa index v√† embedding. Rebuild FAISS...")
            shutil.rmtree(FAISS_INDEX_DIR)
            os.makedirs(FAISS_INDEX_DIR, exist_ok=True)
            vectorstore = build_faiss_from_docs()
    except Exception as e:
        print("‚ùå L·ªói khi load FAISS index:", e)
        print("‚û°Ô∏è X√≥a v√† rebuild l·∫°i FAISS index ƒë·ªÉ an to√†n.")
        if os.path.exists(FAISS_INDEX_DIR):
            shutil.rmtree(FAISS_INDEX_DIR)
            os.makedirs(FAISS_INDEX_DIR, exist_ok=True)
        vectorstore = build_faiss_from_docs()
else:
    # ch∆∞a c√≥ index ‚Üí build m·ªõi
    vectorstore = build_faiss_from_docs()

# -----------------------
# Step 3: Model + Prompt
# -----------------------
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.1, google_api_key=os.getenv("GEMINI_API_KEY"))

prompt_template = PromptTemplate.from_template(
    """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ t∆∞ v·∫•n h∆∞·ªõng nghi·ªáp.
D∆∞·ªõi ƒë√¢y l√† ng·ªØ c·∫£nh t·ª´ t√†i li·ªáu m√† b·∫°n c√≥ th·ªÉ tham kh·∫£o:

{context}

C√¢u h·ªèi: {question}

H√£y tr·∫£ l·ªùi chi ti·∫øt, t·ª± nhi√™n v√† ch√≠nh x√°c. N·∫øu kh√¥ng ƒë·ªß d·ªØ ki·ªán, h√£y ph·∫£n h·ªìi chung mang t√≠nh ƒë·ªãnh h∆∞·ªõng ngh·ªÅ nghi·ªáp."""
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    chain_type_kwargs={"prompt": prompt_template},
)

# -----------------------
# Step 4: H√†m truy v·∫•n
# -----------------------
def answer_question(question: str) -> str:

    """Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi t·ª´ h·ªá th·ªëng RAG (c√≥ debug chi ti·∫øt)."""
    print(f"üß† Nh·∫≠n c√¢u h·ªèi: {question}")
    try:
        result = qa_chain.invoke({"query": question})
        print(f"üß© Raw result type: {type(result)}")
        print(f"üß© Raw result content: {result}")

        if isinstance(result, dict):
            return result.get("result") or result.get("output_text") or str(result)
        elif hasattr(result, "output_text"):
            return result.output_text
        else:
            return str(result)

    except Exception as e:
        import traceback
        print("‚ùå FULL ERROR TRACEBACK:")
        traceback.print_exc()
        print(f"‚ùå Error message: {e}")
        return "Xin l·ªói, h·ªá th·ªëng g·∫∑p l·ªói khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n."



